import argparse
import re
import requests
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor
import os

class DomainRecon:
    def __init__(self):
        self.results = {}
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
    def normalize_url(self, url):
        """Ensure URL has scheme (http/https)"""
        if not url.startswith(('http://', 'https://')):
            return f'http://{url}'
        return url

    def extract_domain(self, url):
        """Extract domain from URL"""
        parsed = urlparse(self.normalize_url(url))
        return parsed.netloc

    def process_file(self, file_path):
        """Process input file containing domains/subdomains"""
        with open(file_path, 'r') as f:
            return [line.strip() for line in f if line.strip()]

    def get_page_content(self, url):
        """Fetch page content with error handling"""
        try:
            resp = self.session.get(url, timeout=10, verify=False)
            return resp.text if resp.status_code == 200 else None
        except:
            return None

    def find_js_files(self, html, base_url):
        """Find all JS files referenced in HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        js_files = []
        
        for script in soup.find_all('script'):
            if script.get('src'):
                js_url = urljoin(base_url, script.get('src'))
                js_files.append(js_url)
                
        return js_files

    def extract_endpoints_from_js(self, js_content):
        """Extract endpoints from JavaScript content"""
        # Common patterns for API endpoints
        patterns = [
            r'[\'\"](/[^\'\"\s?#]+)[\'\" ]',  # '/endpoint'
            r'[\'\"](https?://[^\'\"\s?#]+)[\'\" ]',  # full URLs
            r'\.(get|post|put|delete|patch|fetch)\([\'\"]([^\'\"\s?#]+)[\'\" ]',  # HTTP methods
            r'url:\s*[\'\"]([^\'\"\s?#]+)[\'\" ]',  # 'url: /endpoint'
            r'api/v\d+/[^\'\"\s?#]+',  # API version patterns
        ]
        
        endpoints = set()
        
        for pattern in patterns:
            matches = re.finditer(pattern, js_content)
            for match in matches:
                endpoint = match.group(1) if len(match.groups()) > 0 else match.group(0)
                endpoints.add(endpoint)
                
        return endpoints

    def extract_endpoints_from_html(self, html_content):
        """Extract endpoints from HTML content"""
        soup = BeautifulSoup(html_content, 'html.parser')
        endpoints = set()
        
        # Check links (a tags)
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith(('/', 'http://', 'https://')):
                endpoints.add(href)
                
        # Check forms
        for form in soup.find_all('form', action=True):
            action = form['action']
            if action:
                endpoints.add(action)
                
        return endpoints

    def process_domain(self, domain):
        """Main processing for a single domain"""
        domain = self.extract_domain(domain)
        normalized_url = self.normalize_url(domain)
        base_url = f"{normalized_url.rstrip('/')}/"
        
        print(f"[*] Processing: {domain}")
        
        # Get main page
        main_page = self.get_page_content(normalized_url)
        if not main_page:
            print(f"[-] Failed to fetch main page for {domain}")
            return
            
        # Extract endpoints from HTML
        html_endpoints = self.extract_endpoints_from_html(main_page)
        
        # Find and process JS files
        js_files = self.find_js_files(main_page, base_url)
        js_endpoints = set()
        
        for js_file in js_files:
            js_content = self.get_page_content(js_file)
            if js_content:
                js_endpoints.update(self.extract_endpoints_from_js(js_content))
        
        # Combine all endpoints
        all_endpoints = html_endpoints.union(js_endpoints)
        self.results[domain] = all_endpoints
        
        print(f"[+] Found {len(all_endpoints)} endpoints for {domain}")

    def save_results(self, output_file):
        """Save results to output file"""
        with open(output_file, 'w') as f:
            for domain, endpoints in self.results.items():
                f.write(f"{domain}:\n")
                for endpoint in endpoints:
                    f.write(f"  {endpoint}\n")
                f.write("\n")
        print(f"[*] Results saved to {output_file}")

def main():
    parser = argparse.ArgumentParser(description="Domain Endpoint Reconnaissance Tool")
    parser.add_argument("-d", "--domain", help="Single domain to process")
    parser.add_argument("-f", "--file", help="File containing list of domains/subdomains")
    parser.add_argument("-o", "--output", default="results.txt", help="Output file name")
    parser.add_argument("-t", "--threads", type=int, default=5, help="Number of threads")
    
    args = parser.parse_args()
    
    if not args.domain and not args.file:
        parser.error("Please provide either a domain (-d) or a file (-f)")
    
    recon = DomainRecon()
    
    domains = []
    if args.domain:
        domains.append(args.domain)
    if args.file:
        domains.extend(recon.process_file(args.file))
    
    # Process domains with threading
    with ThreadPoolExecutor(max_workers=args.threads) as executor:
        executor.map(recon.process_domain, domains)
    
    # Save results
    recon.save_results(args.output)

if __name__ == "__main__":
    main()
